#!/bin/bash +x

function log () {
  case $1 in
    "INFO")
      LOG_COLOR="\033[96m";;
    "ERROR")
      LOG_COLOR="\033[31m";;
    "WARN")
      LOG_COLOR="\033[93m" ;;
    "SUCCESS")
      LOG_COLOR="\033[32m" ;;
    *)
      LOG_COLOR="\033[1m" ;;
  esac

  echo -e "$${LOG_COLOR} [$(date)] $${1}: $2\033[0m" | tee -a "/var/log/bootstrap.log"
}

function error () {
  log "ERROR" "$1"
  exit
}

# Python is the best bet to safely escape the log data
# for json consumption. json is a std python lib so 
# minimal dependency requirements
function json_escape () {
    printf '%s' "$1" | python -c 'import json,sys; print(json.dumps(sys.stdin.read()))'
}

export aws="$(which aws || echo '/usr/bin/aws')";
export curl="$(which curl || echo '/usr/bin/curl')";

# Instance ID begets all things...
log 'INFO' 'Retrieving Instance ID from Metadata';
instance_id="$($${curl} -s http://169.254.169.254/latest/meta-data/instance-id)";
if [ $${?} -ne 0 ]; then
  error "Failed to retrieve Instance ID";
fi
log 'INFO' "Instance ID: $${instance_id}";

# Get Current AWS Region in order to do anything useful...
log 'INFO' 'Retrieving Instance Identity Document from Metadata';
instance_identity_document="$($${curl} -s http://169.254.169.254/latest/dynamic/instance-identity/document)";
if [ $${?} -ne 0 ]; then
  error "$${curl} -s http://169.254.169.254/latest/dynamic/instance-identity/document";
else
  log 'INFO' 'Instance Identity Document Retrieved';

  region="$(echo "$${instance_identity_document}" | jq -r .region)";

  if [ $${?} -ne 0 ]; then
    error 'jq: Instance Identity Document could not be parsed for current Region';
  else
    log 'INFO' "Region retrieved: $${region}";
  fi;

  # Make it available to the AWS cli; who needs to specify the region manually on every single call(?)
  export AWS_DEFAULT_REGION="$${region}";
fi;

# Get the Autoscaling Group Name - it's a bit dirty but it works.
asg_name="$($${aws} autoscaling describe-auto-scaling-instances \
  --instance "$${instance_id}" \
  --output json \
  | jq -r .AutoScalingInstances[0].AutoScalingGroupName)";

if [ $${?} -ne 0 ]; then
  error 'Failed to retrieve the autoscaling group name for this instance';
else
  log 'INFO' "ASG name retrieved: $${asg_name}";
fi;

# Get the Launching LifeCycle Hook Name for this ASG (Making the assumption there's only one hook!)
lifecycle_hook_name="$($${aws} autoscaling describe-lifecycle-hooks \
  --auto-scaling-group-name "$${asg_name}" \
  --output json \
  | jq -r '.LifecycleHooks[] | select(.LifecycleTransition=="autoscaling:EC2_INSTANCE_LAUNCHING") | .LifecycleHookName')";

if [ $${?} -ne 0 ]; then
  error "Failed to retrieve lifecycle hook name for the autoscaling:EC2_INSTANCE_LAUNCHING transition for the autoscaling group named: $${asg_name}";
else
  log 'INFO' "Lifecycle hook name retrieved: $${lifecycle_hook_name}"
fi;

# Prerequisite information gathered - begin bootstrapping

BOOTSTRAP_SUCCESS=1

sed -E -i "s/'[a-z0-9]{8}\-[a-z0-9]{4}\-[a-z0-9]{4}\-[a-f0-9]{4}\-[a-z0-9]{12}'/'${KMS_KEY}'/g" /etc/eyaml/config.yaml

# Ensure the EFS directory exists
mkdir -p ${MOUNT_POINT};
mkdir -p ${MOUNT_POINT_ENC};

# Get Region and AZ from AWS EC2 meta-data. We could take region as a template parameter, but why since meta-data is canonical?
export region="$(curl -s http://169.254.169.254/latest/dynamic/instance-identity/document | jq -r .region)";
export az="$(curl -s http://169.254.169.254/latest/meta-data/placement/availability-zone)";

# TODO: peacheym: Migrate this to the AZ-independent URL
# Set mount information in /etc/fstab - we're unlikely to reboot, but it can't hurt to ensure it remains
echo "$${az}.${EFS_ID}.efs.$${region}.amazonaws.com:/ /efs nfs4 nfsvers=4.1,rsize=1048576,wsize=1048576,hard,timeo=600,retrans=2 0 0" >> /etc/fstab
echo "$${az}.${EFS_ENC_ID}.efs.$${region}.amazonaws.com:/ ${MOUNT_POINT_ENC} efs rsize=1048576,wsize=1048576,hard,timeo=600,retrans=2,noresvport,tls 0 0" >> /etc/fstab

# Mount the configured target for Batch
mount ${MOUNT_POINT};
mount ${MOUNT_POINT_ENC};

# Shift this to cloud-init config yaml
facts_path="/opt/puppetlabs/facter/facts.d"
echo -n 'environment: ${ENVIRONMENT}'    > "$${facts_path}/environment.yaml"
echo -n 'nodetype: ${NODETYPE}'          > "$${facts_path}/nodetype.yaml"
echo -n 'aws_account: ${AWS_ACCOUNT_ID}' > "$${facts_path}/aws_account.yaml"

cd /opt/packer-puppet-masterless

log 'INFO' 'Running puppet to configure environment'

/opt/puppetlabs/bin/puppet apply \
  --verbose \
  --detailed-exitcodes \
  --modulepath='/opt/packer-puppet-masterless/module-0:/opt/packer-puppet-masterless/module-1' \
  --hiera_config='/opt/packer-puppet-masterless/hiera.yaml' \
  /opt/packer-puppet-masterless/manifests/site.pp

case $${?} in
  6|4|1)
    log "ERROR" "Puppet run failed with exit code $${?}"
    BOOTSTRAP_SUCCESS=0 ;;
  *)
    log "INFO" "Puppet run completed" ;;
esac

if [ $${BOOTSTRAP_SUCCESS} -eq 1 ]; then
  log 'SUCCESS' 'Bootstrap complete for nodetype: ${NODETYPE}'
else
  log 'ERROR' 'Bootstrap abandon for nodetype: ${NODETYPE}'
fi

sed -i -e "s|production|${ENVIRONMENT}|g" /opt/aws/amazon-cloudwatch-agent/etc/amazon-cloudwatch-agent.toml
/etc/init.d/cloudwatch restart

# Sure would be lovely if date could give unix time 
# in milliseconds
timestamp=$(($(date +'%s * 1000 + %-N / 1000000')))
log_data="$(json_escape "$(cat /var/log/cloud-init-output.log)")"

echo """
[
  {
    \"timestamp\": $${timestamp},
    \"message\": $${log_data}
  }
]""" > /var/log/cwl-cloud-init.txt

aws logs create-log-stream \
  --region $${region} \
  --log-group-name ${LOG_GROUP} \
  --log-stream-name $${instance_id}

aws logs put-log-events \
  --region $${region} \
  --log-group-name ${LOG_GROUP} \
  --log-stream-name $${instance_id} \
  --log-events file:///var/log/cwl-cloud-init.txt

log 'INFO' 'Completing lifecycle action'

if [ $${BOOTSTRAP_SUCCESS} -eq 1 ]; then
  $${aws} autoscaling complete-lifecycle-action \
      --lifecycle-action-result CONTINUE \
      --instance-id "$${instance_id}" \
      --lifecycle-hook-name "$${lifecycle_hook_name}" \
      --auto-scaling-group-name "$${asg_name}" \
      || error 'Failed to complete lifecycle hook';
else
  $${aws} autoscaling complete-lifecycle-action \
    --lifecycle-action-result ABANDON \
    --instance-id "$${instance_id}" \
    --lifecycle-hook-name "$${lifecycle_hook_name}" \
    --auto-scaling-group-name "$${asg_name}" \
    || error 'Failed to complete lifecycle hook';
fi


## TEMPORARY WORK TO GET EFS ENCRYPTED
log 'INFO' 'Starting EFS work'

cat >> /root/efs_sync.sh << EOT
#!/bin/bash
PATH="/usr/local/bin:/sbin:/bin:/usr/sbin:/usr/bin:/opt/puppetlabs/bin"

# Check that EFS is mounted
if [ ! $(df -hT | grep -c nfs4) = 2 ]; then
  echo "EFS not mounted - Aborting !"
  exit 0
fi

# Dont run multiple processes simultainously
if [ -f "/efs_enc/sync.lock" ]; then
  DTE=xDTCx
  echo "   ==> Lock file found (/efs_enc/sync.lock) - Wont start a new sync job : " xDTEx
  exit 0
else 
  # Prevent other processes from starting
  touch "/efs_enc/sync.lock"
fi

# Perform the sync
SRC="/efs"
DST="/efs_enc"
date
echo "Syncing: xSRCx ==> xDSTx"
echo 3 > /proc/sys/vm/drop_caches

if [ -f "xDSTx/.initialsync" ]; then
  echo "Performing supplementary sync using RSYNC"
  THREADS=xZZZx
  echo "Number of threads: " xTHREADSx
  time fpsync -m rsync -o W -n xTHREADSx xSRCx xDSTx
  echo "Supplementary sync using RSYNC completed"
else
  echo "Performing initial sync using CPIO"
  THREADS=xYYYx
  echo "Number of threads: " xTHREADSx
  time fpsync -m cpio -n xTHREADSx xSRCx xDSTx
  date
  touch "xDSTx/.initialsync"
  echo "Initial sync using CPIO completed."
  echo "Checking sizes using DU"
  du -sh xSRCx xDSTx
fi

echo
date
echo "#####################################################"
echo

# Remove the lock
rm -f "/efs_enc/sync.lock"
EOT

sed -i 's/xSRCx/\$SRC/g' /root/efs_sync.sh
sed -i 's/xDSTx/\$DST/g' /root/efs_sync.sh
sed -i 's/xDTEx/\$DTE/g' /root/efs_sync.sh
sed -i 's/xDTCx/\$(date +%d%h%Y:%H%M)/g' /root/efs_sync.sh 
sed -i 's/xTHREADSx/\$THREADS/g' /root/efs_sync.sh
sed -i 's/xZZZx/\$((\$(nproc --all) * 4))/g' /root/efs_sync.sh         # 4 threads
sed -i 's/xYYYx/\$(((\$(nproc --all) * 4) * 16))/g' /root/efs_sync.sh  # 64 threads

chmod +x /root/efs_sync.sh

crontab -l > /tmp/cron_tmp
echo >> /tmp/cron_tmp
echo "# Temporary cron for keeping EFS in sync" >> /tmp/cron_tmp
#echo "00 4,10,16,22 * * * /root/efs_sync.sh >> /var/log/efs_sync.log 2>&1" >> /tmp/cron_tmp
echo "*/15 */1 * * * /root/efs_sync.sh >> /var/log/efs_sync.log 2>&1" >> /tmp/cron_tmp
crontab /tmp/cron_tmp
rm -f /tmp/cron_tmp

/root/efs_sync.sh >> /var/log/efs_sync.log